{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8341c613-0bfa-4bf7-8fb0-f54c1818b02c",
   "metadata": {},
   "source": [
    "# Lab 2: Document Processing with PaddleOCR\n",
    "\n",
    "In this lesson, you'll learn how OCR has evolved over time from Tesseract to PaddleOCR.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use PaddleOCR for text detection and text recognition\n",
    "- Identify PaddleOCR limitations on charts and multi-column layouts\n",
    "- Use PaddleOCR for layout detection to identify regions of interest\n",
    "\n",
    "## Background\n",
    "\n",
    "Tesseract represents the traditional computer vision era. It uses hand-engineered features and procedural rules. PaddleOCR represents the modern deep learning era. It uses neural networks in two steps:\n",
    "\n",
    "1. **Text Detection**: Locate all text regions in the image\n",
    "2. **Text Recognition**: Decode the text content from each region\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [1. PaddleOCR Basics](#1)\n",
    "  - [1.1. Create the PaddleOCR Tool](#1-1)\n",
    "  - [1.2. Create & Run the Agent](#1-2)\n",
    "  - [1.3. Run PaddleOCR on Table and Handwriting Examples](#1-3)\n",
    "- [2. PaddleOCR Limitations](#2)\n",
    "- [3. PaddleOCR Layout Detection](#3)\n",
    "- [4. More Complex Layout](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-note",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the packages for the OCR pipeline:\n",
    "- **Pillow** for image loading\n",
    "- **OpenCV** for image processing\n",
    "- **PaddleOCR** for parsing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874b304-c3b3-49bc-9abe-df7e026b9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "os.environ['DISABLE_MODEL_SOURCE_CHECK'] = 'True'\n",
    "from paddleocr import PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9027b0c-d05f-40b4-969d-dc3c72fbfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. PaddleOCR Basics\n",
    "\n",
    "Initialize PaddleOCR with English language support. This loads two models:\n",
    "- **_DET**: Text detection model (locates text regions)\n",
    "- **_REC**: Text recognition model (reads characters)\n",
    "\n",
    "PaddleOCR handles preprocessing and orchestrates both models automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef367d2-5a0a-4001-9017-f9b218c82fab",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Model Download)</code>:</b> We must download both models for PaddleOCR. Be aware that the downloads will take a moment. Subsequently, the models will load from cached files in the `.paddlex` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f84f94-b190-4c98-88fe-a4c0a3f64be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize English OCR model\n",
    "ocr = PaddleOCR(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "receipt-note",
   "metadata": {},
   "source": [
    "This receipt from Lesson 1 will show how PaddleOCR compares to Tesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1874b7-ad54-4221-9dc4-2b58ce7b0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'receipt.jpg'\n",
    "img = Image.open(image_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-ocr-note",
   "metadata": {},
   "source": [
    "Run OCR on the image. The result contains one page-level dictionary for single-page images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e420d9-277d-467c-98e8-7c63e85ee6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR\n",
    "result = ocr.predict(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-note",
   "metadata": {},
   "source": [
    "Print the OCR results containing:\n",
    "- **Recognized text** strings\n",
    "- **Confidence scores** for each line\n",
    "- **Bounding box coordinates** (localization information)\n",
    "\n",
    "Note two improvements over Tesseract: (1) bounding boxes showing where each text appears, and (2) correct reading of \\\\$7.95. Previously, Tesseract misread this as \\\\$7.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a033e-a0cf-469d-ba56-54fe5c2442e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = result[0]\n",
    "texts = page['rec_texts'] # recognized text strings\n",
    "scores = page['rec_scores'] # confidence scores for each text line\n",
    "boxes  = page['rec_polys'] #  bounding box coordinates\n",
    "\n",
    "for text, score, box in zip(texts, scores, boxes):\n",
    "    # box is a numpy array like [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "    coords = box.astype(int).tolist()  # convert to normal list of ints\n",
    "    print(f\"{text:25} | {score:.3f} | {coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-note",
   "metadata": {},
   "source": [
    "PaddleOCR preprocesses images automatically‚Äîcorrecting rotation, deskewing, and removing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde51d4-d349-485c-a3e4-005b2eed3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = page['doc_preprocessor_res']['output_img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fd36b-4cea-4838-ba8f-8b3dd05929d3",
   "metadata": {},
   "source": [
    "The visualization below shows the preprocessed image with bounding boxes and recognized text. Compare to the original: the background is cleaner and the image is slightly rotated for better alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38315036-baf3-4c3a-ad9b-9117d4fa0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plot = img.copy()\n",
    "\n",
    "for text, box in zip(texts, boxes):\n",
    "    pts = np.array(box, dtype=int)\n",
    "    cv2.polylines(img_plot, [pts], True, (0, 255, 0), 2)\n",
    "    x, y = pts[0]\n",
    "    cv2.putText(img_plot, text, (x, y - 5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Aligned Bounding Boxes (Processed Image)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-1",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "\n",
    "### 1.1. Create the PaddleOCR Tool\n",
    "\n",
    "Convert PaddleOCR into a LangChain tool (similar to Lesson 1). The tool returns text, bounding boxes, and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316eefa0-aed4-44a5-9a49-d67b00280dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def paddle_ocr_read_document(image_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reads an image from the given path and returns extracted text \n",
    "    with bounding boxes.\n",
    "    \n",
    "    Returns a list of dictionaries, each containing:\n",
    "    - 'text': the recognized text string\n",
    "    - 'bbox': bounding box coordinates [x_min, y_min, x_max, y_max]\n",
    "    - 'confidence': recognition confidence score (if available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = ocr.predict(image_path)\n",
    "        page = result[0]\n",
    "        \n",
    "        texts = page['rec_texts'] \n",
    "        boxes = page['dt_polys']         \n",
    "        scores = page.get('rec_scores', [None] * len(texts))  \n",
    "        \n",
    "        extracted_items = []\n",
    "        for text, box, score in zip(texts, boxes, scores):\n",
    "            x_coords = [point[0] for point in box]\n",
    "            y_coords = [point[1] for point in box]\n",
    "            bbox = [min(x_coords), min(y_coords), max(x_coords), \n",
    "                    max(y_coords)]\n",
    "            \n",
    "            item = {\n",
    "                'text': text,\n",
    "                'bbox': bbox,\n",
    "            }\n",
    "            if score is not None:\n",
    "                item['confidence'] = score\n",
    "                \n",
    "            extracted_items.append(item)\n",
    "        \n",
    "        return extracted_items\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Error reading image: {e}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-2",
   "metadata": {},
   "source": [
    "<a id=\"1-2\"></a>\n",
    "\n",
    "### 1.2. Create & Run the Agent\n",
    "\n",
    "Configure the agent with PaddleOCR as its tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769a453-e6c2-4b6d-bbbc-42760b5a3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the list of tools\n",
    "tools = [paddle_ocr_read_document]\n",
    "\n",
    "# 2. Set up the OpenAI GPT model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\", \n",
    "    temperature=1 \n",
    ")\n",
    "\n",
    "# 3. Create the OpenAI-compatible prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant designed to extract information \"+\n",
    "        \"from documents. \"\n",
    "            \"You have access to this tool: \"\n",
    "            \"Paddle OCR tool to extract raw texts, bounding boxes for \"+\n",
    "            \"each text and confidence score from images \"\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Create a proper tool-calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Set up the AgentExecutor to run the tool-enabled loop\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "receipt-task-note",
   "metadata": {},
   "source": [
    "Verify the receipt total. The turquoise output shows PaddleOCR results, and the green output shows LLM reasoning.\n",
    "\n",
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> üö®\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI can vary with each execution due to their non-deterministic nature. Don't be surprised if your results differ slightly from those shown in the video.</p>\n",
    "\n",
    "With correct OCR inputs (unlike Tesseract's misreads), the total verification succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9730a10-df34-4fdc-9df7-dac77bc9bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Process the document at 'receipt.jpg' and evaluate that \n",
    "the total is correct.\n",
    "\"\"\"\n",
    "response = agent_executor.invoke({\"input\": task})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "receipt-result-note",
   "metadata": {},
   "source": [
    "This demonstrates how accurate OCR (PaddleOCR) combined with LLM reasoning produces correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-3",
   "metadata": {},
   "source": [
    "<a id=\"1-3\"></a>\n",
    "\n",
    "### 1.3. Run PaddleOCR on the Table and Handwriting Examples\n",
    "\n",
    "Test PaddleOCR on the table and handwriting examples from Lesson 1.\n",
    "\n",
    "This helper function runs OCR and visualizes results with bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff187ef8-e6e0-4115-a402-70fd43e9ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocr(image_path, ocr_model = ocr, show_text = True):\n",
    "\n",
    "    display(Image.open(image_path))\n",
    "    result = ocr.predict(image_path)\n",
    "    \n",
    "    page = result[0]\n",
    "    texts  = page['rec_texts'] \n",
    "    scores = page['rec_scores'] \n",
    "    boxes  = page['rec_polys'] \n",
    "    \n",
    "    for text, score, box in zip(texts, scores, boxes):\n",
    "        coords = box.astype(int).tolist()  \n",
    "        print(f\"{text:25} | {score:.3f} | {coords}\")\n",
    "        \n",
    "    img = page['doc_preprocessor_res']['output_img'] \n",
    "    img_plot = img.copy()\n",
    "\n",
    "    image_path = Path(image_path)\n",
    "    output_path = image_path.with_stem(image_path.stem + \"_output\")  \n",
    "    cv2.imwrite(str(output_path), img)\n",
    "    \n",
    "    for text, box in zip(texts, boxes):\n",
    "        pts = np.array(box, dtype=int)\n",
    "        cv2.polylines(img_plot, [pts], True, (0, 255, 0), 2)\n",
    "        x, y = pts[0]\n",
    "        if show_text:\n",
    "            cv2.putText(img_plot, text, \n",
    "                        (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.6, (255, 0, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(8, 10))  \n",
    "    plt.imshow(cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Aligned Bounding Boxes (Processed Image)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-note",
   "metadata": {},
   "source": [
    "#### Table Example\n",
    "\n",
    "Inspect the output for errors. Notice exponential notation issues‚Äî`10^20` may be read as `1020`. This affects both PaddleOCR and Tesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03edab0-dee3-402c-87a2-9f0320aa0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ocr(\"table.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "table-task-note",
   "metadata": {},
   "source": [
    "Extract FLOPs from the EN-DE column. Notice:\n",
    "- **ByteNet** and **Deep-Att** correctly marked as \"not found\" (blank cells)\n",
    "- Scientific notation corrected (e.g., `1020` ‚Üí `10^20`)\n",
    "\n",
    "This demonstrates LLM reasoning‚Äîrecognizing that FLOPs (floating point operations) must be large numbers, not small integers like 1020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185464ee-dffc-4378-be4e-ce321f5bd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Extract the Training Cost (FLOPs) for EN-DE for ALL methods from \n",
    "the table.png using the OCR tool.\n",
    "Return as a list with model name and its training cost.\n",
    "\"\"\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": task})\n",
    "\n",
    "# Display results side by side\n",
    "print(\"\\n\" + \"‚îÄ\"*35 + \" LLM RESULT \" + \"‚îÄ\"*33)\n",
    "print(\"=\"*80)\n",
    "print(response[\"output\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handwriting-note",
   "metadata": {},
   "source": [
    "#### Handwriting Example\n",
    "\n",
    "Test PaddleOCR on student handwriting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d396c13-bb35-4cad-99d2-79ac2773d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ocr(\"handwritten.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf085e-a265-4721-93a5-e63e993a7425",
   "metadata": {},
   "source": [
    "The prompt instructs the agent not to correct grammatical errors, preserving the student's original answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123676d7-e30f-4826-92b7-910aa29bc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "Please process the document at 'handwritten.jpg' using ocr \n",
    "and extract the following information in JSON format:\n",
    "- `student name`\n",
    "- `student answer to all the ten questions`\n",
    "Do not correct any grammatical errors.\n",
    "\"\"\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": task})\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*35 + \" LLM RESULT \" + \"‚îÄ\"*33)\n",
    "print(response[\"output\"])\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handwriting-result-note",
   "metadata": {},
   "source": [
    "Note that grammatical error were preserved in results. For example, Question 9 has \"is\" instead of \"are\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. PaddleOCR Limitations\n",
    "\n",
    "PaddleOCR had few errors than Tesseract. However handwriting remained challenging. Let's test PaddleOCR on other challenging cases to expose its limitations.\n",
    "\n",
    "### Charts and Figures\n",
    "\n",
    "This `report.png` document contains a table (top), text (center), and a **line chart** with caption (bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ec7e-221b-4b03-846e-ad2df26367b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ocr(\"report.png\", show_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-result-note",
   "metadata": {},
   "source": [
    "PaddleOCR did not recognize the chart as a region. Note the lack of a bounding box around it. While PaddleOCR detected labels on axes, there is no context for these label. For example, horizontal axis versus vertical axis. \n",
    "\n",
    "### Multi-Column Layouts\n",
    "\n",
    "This `article.jpg` shows a typical academic article with multiple columns, an abstract, callout boxes, and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dad66-2904-4d31-b035-b8c98f115eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ocr(\"article.jpg\", show_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "article-result-note",
   "metadata": {},
   "source": [
    "PaddleOCR reads straight across all columns as opposed to down the left column, then the next column.\n",
    "\n",
    "For example \"in most of the Westernized countries that undertake...\" becomes \"in most of the Westernized countries system based on some of the interview...\"\n",
    "\n",
    "This produces garbled text in multi-column layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. PaddleOCR Layout Detection\n",
    "\n",
    "PaddleOCR includes a layout detection module that identifies document regions (footer, tables, charts, etc) before text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002d79c-db19-4096-980f-5d147757cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import LayoutDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa971c-78c7-4240-89c5-bf891bb4f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_engine = LayoutDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layout-function-note",
   "metadata": {},
   "source": [
    "This function processes images with the layout engine, returning:\n",
    "- **label**: Region type (text, chart, table, etc.)\n",
    "- **score**: Confidence score\n",
    "- **bbox**: Bounding box coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378023ee-92c3-4f01-88fb-ee520b6be4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(image_path):\n",
    "    # Get layout regions\n",
    "    layout_result = layout_engine.predict(image_path)\n",
    "    \n",
    "    # Parse the boxes\n",
    "    regions = []\n",
    "    for box in layout_result[0]['boxes']:\n",
    "        regions.append({\n",
    "            'label': box['label'],\n",
    "            'score': box['score'],\n",
    "            'bbox': box['coordinate'],  # [x1, y1, x2, y2]\n",
    "        })\n",
    "    \n",
    "    # Sort by confidence\n",
    "    regions = sorted(regions, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layout-report-note",
   "metadata": {},
   "source": [
    "Apply layout detection to the economic report. The labels identify region types: **text**, **chart**, **paragraph_title**, **table**, **footer**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13a7a0-445c-4a13-940d-c80e3f42e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = process_document(\"report.png\")\n",
    "\n",
    "for r in regions:\n",
    "    print(f\"{r['label']:20} score: {r['score']:.3f}  bbox: {[int(x) for x in r['bbox']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-note",
   "metadata": {},
   "source": [
    "Visualize layout detection results with color-coded bounding boxes for each region type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c7ba2-b525-4a06-851a-cfdc408b8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layout(image_path, min_confidence=0.5):\n",
    "    \n",
    "    layout_result = layout_engine.predict(image_path)\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    img_plot = img.copy()\n",
    "    \n",
    "    # Get all unique labels\n",
    "    labels = list(set(box['label'] for box in layout_result[0]['boxes']))\n",
    "    \n",
    "    # Generate colors dynamically from colormap\n",
    "    cmap = colormaps.get_cmap('tab20') \n",
    "    color_map = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        rgba = cmap(i % 20)\n",
    "        # Convert to BGR (0-255) for OpenCV\n",
    "        color_map[label] = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))\n",
    "    \n",
    "    for box in layout_result[0]['boxes']:\n",
    "        if box['score'] < min_confidence:\n",
    "            continue\n",
    "            \n",
    "        label = box['label']\n",
    "        score = box['score']\n",
    "        coords = box['coordinate']\n",
    "        \n",
    "        color = color_map[label]\n",
    "        \n",
    "        x1, y1, x2, y2 = [int(c) for c in coords]\n",
    "        pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype=int)\n",
    "        \n",
    "        cv2.polylines(img_plot, [pts], True, color, 2)\n",
    "        text = f\"{label} ({score:.2f})\"\n",
    "        cv2.putText(img_plot, text, (x1, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    \n",
    "    return img_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-layout-note",
   "metadata": {},
   "source": [
    "Visualize layout detection on the economic report. Notice:\n",
    "- Multiple **text** blocks\n",
    "- **Paragraph_title** and **table** regions\n",
    "- **Chart** with a complete bounding box (not just axis labels)\n",
    "- Small elements: **number** and **footer**\n",
    "\n",
    "The layout model correctly identifies all major document regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dafcc-39c8-4c01-8e77-3c0335368af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "result_image = visualize_layout(\"report.png\", min_confidence=0.5)\n",
    "\n",
    "# Display with matplotlib\n",
    "plt.figure(figsize=(12, 14))\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Layout Detection Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "article-layout-note",
   "metadata": {},
   "source": [
    "Apply layout detection to the multi-column article. Notice region labels:\n",
    "- **text**, **paragraph_title**, **document_title**, **abstract**\n",
    "- **footnote**, **footer**, **table**\n",
    "\n",
    "Layout detection preserves column boundaries. Text blocks stay intact which prevents the reading order errors seen earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e13fa-133f-4a28-845f-f1c22463e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "result_image = visualize_layout(\"article.jpg\", min_confidence=0.5)\n",
    "\n",
    "# Display with matplotlib\n",
    "plt.figure(figsize=(12, 14))\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Layout Detection Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. More Complex Layout\n",
    "\n",
    "Test layout detection on a bank statement which requires extracting key-value pairs from complex tabular layouts.\n",
    "\n",
    "- Table header detection accuracy\n",
    "- Whether multiple tables are distinguished\n",
    "- Small text (legal disclaimers) detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f9619-1c9f-431b-a3bc-8180f9839d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "result_image = visualize_layout(\"bank_statement.png\", min_confidence=0.5)\n",
    "\n",
    "# Display with matplotlib\n",
    "plt.figure(figsize=(15, 20))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.title('Layout Detection Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's what we covered:\n",
    "\n",
    "| Aspect | PaddleOCR | Tesseract |\n",
    "|--------|-----------|----------|\n",
    "| **Era** | Deep Learning | Traditional/Procedural |\n",
    "| **Approach** | Neural networks | Hand-engineered rules |\n",
    "| **Strength** | Real-world documents, irregular text | Clean printed text |\n",
    "| **Output** | Text + bounding boxes + confidence scores | Text only |\n",
    "\n",
    "The next lesson explores layout detection and reading order in depth to handle some of the key challenges around charts and tables. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
