{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# Lab 3: Building Agentic Document Understanding\n",
    "\n",
    "In this lesson, you will build an agent that combines tools for OCR, layout detection, reading order and Vision-Language Model (VLM).\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use PaddleOCR for text parsing and layout detection\n",
    "- Use LayoutReader for sorting parsed text into reading order \n",
    "- Build VLM tools for chart and table analysis\n",
    "\n",
    "## Background\n",
    "\n",
    "LayoutReader is a model for determining reading order. By sorting information on each page, the model captures the logical sequence of text parsed from the document. For documents with multiple columns, floating captions, margin annotations, etc the reading order can be complex. \n",
    "\n",
    "- Input: Bounding boxes normalized to 0-1000 range\n",
    "- Output: Reading order position for each box\n",
    "\n",
    "LayoutReader uses LayoutLMv3 which was developed by Microsoft on the ReadingBank dataset (500,000+ annotated pages).\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [1. Text Extraction with PaddleOCR + LayoutLM Ordering](#1)\n",
    "  - [1.1. Running OCR on the Document](#1-1)\n",
    "  - [1.2. Visualizing OCR Bounding Boxes](#1-2)\n",
    "  - [1.3. Structuring OCR Results with a Dataclass](#1-3)\n",
    "  - [1.4. LayoutLM Reading Order](#1-4)\n",
    "  - [1.5. Visualizing the Reading Order](#1-5)\n",
    "  - [1.6. Creating the Ordered Text Output](#1-6)\n",
    "- [2. Layout Detection with PaddleOCR](#2)\n",
    "  - [2.1. Processing Document Layout](#2-1)\n",
    "  - [2.2. Structuring Layout Results](#2-2)\n",
    "  - [2.3. Visualizing Layout Detection](#2-3)\n",
    "  - [2.4. Cropping Regions for Agent Tools](#2-4)\n",
    "- [3. Agent Tools](#3)\n",
    "  - [3.1. VLM Helper and Prompts](#3-1)\n",
    "  - [3.2. Creating the AnalyzeChart Tool](#3-2)\n",
    "  - [3.3. Creating the AnalyzeTable Tool](#3-3)\n",
    "  - [3.4. Testing the Tools](#3-4)\n",
    "- [4. LangChain Agent](#4)\n",
    "  - [4.1. Formatting Context for the Agent](#4-1)\n",
    "  - [4.2. Creating the System Prompt](#4-2)\n",
    "  - [4.3. Assembling the Agent](#4-3)\n",
    "  - [4.4. Testing the Agent](#4-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df53c38-8551-4953-b4b3-1300f01a1692",
   "metadata": {},
   "source": [
    "## Architecture Overview \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"architecture.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-note",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load environment variables including the OpenAI API key for VLM tools and agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfa9ce-a207-4ba8-a368-688a61ffbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-note",
   "metadata": {},
   "source": [
    "Import libraries:\n",
    "- **Pillow** for image loading and manipulation\n",
    "- **cv2 (OpenCV)** for image processing and bounding box visualization\n",
    "- **matplotlib** for result visualization\n",
    "- **numpy** for numerical operations on arrays\n",
    "- **dataclass** for structured data storage\n",
    "- **typing** for type hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe79b-eb9a-4354-a7eb-0e965a794bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Text Extraction with PaddleOCR + LayoutLM Ordering\n",
    "\n",
    "Extract text and determine reading order using PaddleOCR and LayoutLM.\n",
    "\n",
    "PaddleOCR returns three components for each detected text region:\n",
    "- **Recognized text** strings\n",
    "- **Confidence scores**\n",
    "- **Bounding box coordinates** (4-point polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb88358-7f11-4185-bf71-0b8e4f0c0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# Initialize PaddleOCR \n",
    "ocr = PaddleOCR(lang='en')\n",
    "\n",
    "# Load image\n",
    "image_path = \"report_original.png\"\n",
    "display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-1",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "\n",
    "### 1.1. Running OCR on the Document\n",
    "\n",
    "Run OCR on the document (the same economics report from previous lessons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea733fb-4ea5-4b9d-83fd-e918e3b35464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR \n",
    "result = ocr.predict(image_path)\n",
    "page = result[0]\n",
    "\n",
    "texts = page['rec_texts']      # recognized text strings\n",
    "scores = page['rec_scores']    # confidence scores\n",
    "boxes = page['rec_polys']      # bounding box coordinates\n",
    "\n",
    "print(f\"Extracted {len(texts)} text regions\")\n",
    "print(\"\\nFirst 10 regions:\")\n",
    "for text, score, box in list(zip(texts, scores, boxes))[:10]:\n",
    "    coords = box.astype(int).tolist()\n",
    "    print(f\"{text:40} | {score:.3f} | {coords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-2",
   "metadata": {},
   "source": [
    "<a id=\"1-2\"></a>\n",
    "\n",
    "### 1.2. Visualizing OCR Bounding Boxes\n",
    "\n",
    "Draw bounding boxes to verify OCR detection. Each text line, table cell, and label gets its own box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04368a-5abb-487b-9ebb-4ed22883a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_img = page['doc_preprocessor_res']['output_img']\n",
    "img_plot = processed_img.copy()\n",
    "show_text= False\n",
    "\n",
    "for text, box in zip(texts, boxes):\n",
    "    pts = np.array(box, dtype=int)\n",
    "    cv2.polylines(img_plot, [pts], True, (0, 255, 0), 2)\n",
    "    x, y = pts[0]\n",
    "    if show_text:\n",
    "        cv2.putText(img_plot, text, (x, y - 5), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Aligned Bounding Boxes (Processed Image)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-3",
   "metadata": {},
   "source": [
    "<a id=\"1-3\"></a>\n",
    "\n",
    "### 1.3. Structuring OCR Results with a Dataclass\n",
    "\n",
    "Structure OCR output using an `OCRRegion` dataclass for cleaner code:\n",
    "- Typed structure for each text region\n",
    "- `bbox_xyxy` property converts 4-point polygons to `[x1, y1, x2, y2]` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff1daf-9a44-4851-bc56-c44397c9580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store OCR results in a structured format\n",
    "@dataclass\n",
    "class OCRRegion:\n",
    "    text: str\n",
    "    bbox: list  # [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]\n",
    "    confidence: float\n",
    "    \n",
    "    @property\n",
    "    def bbox_xyxy(self):\n",
    "        \"\"\"Return bbox as [x1, y1, x2, y2] format.\"\"\"\n",
    "        x_coords = [p[0] for p in self.bbox]\n",
    "        y_coords = [p[1] for p in self.bbox]\n",
    "        return [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]\n",
    "\n",
    "ocr_regions: List[OCRRegion] = []\n",
    "for text, score, box in zip(texts, scores, boxes):\n",
    "    ocr_regions.append(OCRRegion(\n",
    "        text=text, \n",
    "        bbox=box.astype(int).tolist(), \n",
    "        confidence=score\n",
    "    ))\n",
    "\n",
    "print(f\"Stored {len(ocr_regions)} OCR regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-4",
   "metadata": {},
   "source": [
    "<a id=\"1-4\"></a>\n",
    "\n",
    "### 1.4. LayoutLM Reading Order\n",
    "\n",
    "Simple ordering (eg top-to-bottom, left-to-right) does not apply to our complex document. We will use LayoutReader which itself uses LayoutLMv3 model. Hugging Face contains the LayoutLMv3 model. Additionally we use helper functions for LayoutReader available at this [repository](https://github.com/ppaanngggg/layoutreader.git). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e8b58-bc52-4702-8ab5-15bb2e17022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "from layoutreader.v3.helpers import prepare_inputs, boxes2inputs, parse_logits\n",
    "\n",
    "# Load LayoutReader model\n",
    "print(\"Loading LayoutReader model...\")\n",
    "model_slug = \"hantian/layoutreader\"\n",
    "layout_model = LayoutLMv3ForTokenClassification.from_pretrained(model_slug)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reading-order-function-note",
   "metadata": {},
   "source": [
    "Now implement a reading order function called `get_reading_order`: \n",
    "\n",
    "1. **Calculate image dimensions** - Estimate size from bounding boxes with 10% padding\n",
    "2. **Normalize coordinates** - Scale boxes to 0-1000 range for LayoutLM\n",
    "3. **Prepare inputs** - Convert to transformer format\n",
    "4. **Run inference** - Get model predictions\n",
    "5. **Parse results** - Extract reading order from output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606506c3-0e0c-4ed2-a34b-1d7764abe563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reading_order(ocr_regions):\n",
    "    \"\"\"\n",
    "    Use LayoutReader to determine reading order of OCR regions.\n",
    "    Returns list of reading order positions for each region index.\n",
    "    \"\"\"\n",
    "    # 1. Calculate image dimensions from bounding boxes (with padding)\n",
    "    max_x = max_y = 0\n",
    "    for region in ocr_regions:\n",
    "        x1, y1, x2, y2 = region.bbox_xyxy\n",
    "        max_x = max(max_x, x2)\n",
    "        max_y = max(max_y, y2)\n",
    "\n",
    "    image_width = max_x * 1.1   # Add 10% padding\n",
    "    image_height = max_y * 1.1\n",
    "\n",
    "    # 2. Convert bboxes to LayoutReader format (normalized to 0-1000)\n",
    "    boxes = []\n",
    "    for region in ocr_regions:\n",
    "        x1, y1, x2, y2 = region.bbox_xyxy\n",
    "        # Normalize to 0-1000 range\n",
    "        left = int((x1 / image_width) * 1000)\n",
    "        top = int((y1 / image_height) * 1000)\n",
    "        right = int((x2 / image_width) * 1000)\n",
    "        bottom = int((y2 / image_height) * 1000)\n",
    "        boxes.append([left, top, right, bottom])\n",
    "\n",
    "    # 3. Prepare inputs\n",
    "    inputs = boxes2inputs(boxes)\n",
    "    inputs = prepare_inputs(inputs, layout_model)\n",
    "    \n",
    "    # 4. Run inference\n",
    "    logits = layout_model(**inputs).logits.cpu().squeeze(0)\n",
    "    \n",
    "    # 5. Parse the model's outputs to get reading order\n",
    "    reading_order = parse_logits(logits, len(boxes))\n",
    "\n",
    "    return reading_order\n",
    "\n",
    "# Get reading order\n",
    "reading_order = get_reading_order(ocr_regions)\n",
    "\n",
    "print(f\"Reading order determined for {len(reading_order)} regions\")\n",
    "print(f\"First 20 positions: {reading_order[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-5",
   "metadata": {},
   "source": [
    "<a id=\"1-5\"></a>\n",
    "\n",
    "### 1.5. Visualizing the Reading Order\n",
    "\n",
    "Visualize reading order with numbered overlays on each region. Numbers follow the predicted reading sequence.\n",
    "\n",
    "> **Note:** The model may produce some non-sequential jumps. For complex documents, you may need to fine-tune a custom layout model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d5c2a-6e78-4feb-b650-70cdc917764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_reading_order(ocr_regions, image_array, reading_order, title=\"Reading Order\"):\n",
    "    \"\"\"\n",
    "    Visualize OCR regions with their reading order numbers using matplotlib.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 14))\n",
    "    ax.imshow(cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Create order mapping: index -> reading order position\n",
    "    order_map = {i: order for i, order in enumerate(reading_order)}\n",
    "    \n",
    "    for i, region in enumerate(ocr_regions):\n",
    "        bbox = region.bbox\n",
    "        if bbox and len(bbox) >= 4:\n",
    "            # Draw polygon\n",
    "            ax.add_patch(patches.Polygon(bbox, linewidth=2, \n",
    "                                         edgecolor='blue',\n",
    "                                         facecolor='none', alpha=0.7))\n",
    "            # Add reading order number at center\n",
    "            xs = [p[0] for p in bbox]\n",
    "            ys = [p[1] for p in bbox]\n",
    "            ax.text(sum(xs)/len(xs), sum(ys)/len(ys), \n",
    "                    str(order_map.get(i, i)),\n",
    "                    fontsize=13, color='red', \n",
    "                    ha='center', va='center', fontweight='bold')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_reading_order(ocr_regions, processed_img, \n",
    "                        reading_order, \"LayoutLM Reading Order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-6",
   "metadata": {},
   "source": [
    "<a id=\"1-6\"></a>\n",
    "\n",
    "### 1.6. Creating the Ordered Text Output\n",
    "\n",
    "Combine OCR text with reading order:\n",
    "1. Pair each region with its reading position\n",
    "2. Sort by position\n",
    "3. Return structured list with position, text, confidence, and bbox\n",
    "\n",
    "This ordered text provides agent context for answering text-based questions without VLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e43103-aafa-41cb-834e-7bad8fa9f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ordered text content\n",
    "def get_ordered_text(ocr_regions, reading_order):\n",
    "    \"\"\"\n",
    "    Return OCR regions sorted by reading order\n",
    "    with their text and confidence.\n",
    "    \"\"\"\n",
    "    # 1. Create (reading_position, index, region) tuples and sort\n",
    "    indexed_regions = [(reading_order[i], \n",
    "                        i, \n",
    "                        ocr_regions[i]) for i in range(len(ocr_regions))]\n",
    "    \n",
    "    # 2. Sort by reading position\n",
    "    indexed_regions.sort(key=lambda x: x[0])  \n",
    "    \n",
    "    # 3. Extract ordered text info\n",
    "    ordered_text = []\n",
    "    for position, original_idx, region in indexed_regions:\n",
    "        ordered_text.append({\n",
    "            \"position\": position,\n",
    "            \"text\": region.text,\n",
    "            \"confidence\": region.confidence,\n",
    "            \"bbox\": region.bbox_xyxy\n",
    "        })\n",
    "    \n",
    "    return ordered_text\n",
    "\n",
    "ordered_text = get_ordered_text(ocr_regions, reading_order)\n",
    "\n",
    "print(\"Text in reading order:\")\n",
    "print(\"=\" * 70)\n",
    "ordered_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Layout Detection with PaddleOCR\n",
    "\n",
    "Beyond text extraction, identify **content types** using layout detection.\n",
    "\n",
    "PaddleOCR's `LayoutDetection` identifies document structure. Each region includes:\n",
    "- **label**: Content type (text, table, chart, figure, etc.)\n",
    "- **score**: Confidence score\n",
    "- **bbox**: Bounding box in XYXY format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f7247-21ba-46c3-b6d2-8222ec712af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import LayoutDetection\n",
    "\n",
    "# Initialize layout detection \n",
    "layout_engine = LayoutDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-1",
   "metadata": {},
   "source": [
    "<a id=\"2-1\"></a>\n",
    "\n",
    "### 2.1. Processing Document Layout\n",
    "\n",
    "Run layout detection to identify content types (text blocks, charts, titles, tables, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632add6-f75d-47b8-a481-0eb139986d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process document layout \n",
    "def process_document(image_path):\n",
    "    \"\"\"Get layout regions from document.\"\"\"\n",
    "    layout_result = layout_engine.predict(image_path)\n",
    "    \n",
    "    regions = []\n",
    "    for box in layout_result[0]['boxes']:\n",
    "        regions.append({\n",
    "            'label': box['label'],\n",
    "            'score': box['score'],\n",
    "            'bbox': box['coordinate'],  # [x1, y1, x2, y2]\n",
    "        })\n",
    "    \n",
    "    # Sort by confidence\n",
    "    regions = sorted(regions, key=lambda x: x['score'], reverse=True)\n",
    "    return regions\n",
    "\n",
    "layout_results = process_document(image_path)\n",
    "\n",
    "print(f\"Detected {len(layout_results)} layout regions:\")\n",
    "for r in layout_results:\n",
    "    print(f\"  {r['label']:20} score: {r['score']:.3f}  bbox: {[int(x) for x in r['bbox']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layout-results-note",
   "metadata": {},
   "source": [
    "Top five detected regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a4304-1bc2-45e4-96eb-8ef369dfa3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_results[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-2",
   "metadata": {},
   "source": [
    "<a id=\"2-2\"></a>\n",
    "\n",
    "### 2.2. Structuring Layout Results\n",
    "\n",
    "Create a `LayoutRegion` dataclass with unique IDs for tool references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac4c39-da45-4305-985a-3341d5cd9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LayoutRegion:\n",
    "    region_id: int\n",
    "    region_type: str\n",
    "    bbox: list  # [x1, y1, x2, y2]\n",
    "    confidence: float\n",
    "    \n",
    "# Store layout regions in structured format\n",
    "layout_regions: List[LayoutRegion] = []\n",
    "for i, r in enumerate(layout_results):\n",
    "    layout_regions.append(LayoutRegion(\n",
    "        region_id=i,\n",
    "        region_type=r['label'],\n",
    "        bbox=[int(x) for x in r['bbox']],\n",
    "        confidence=r['score']\n",
    "    ))\n",
    "\n",
    "print(f\"Stored {len(layout_regions)} layout regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-3",
   "metadata": {},
   "source": [
    "<a id=\"2-3\"></a>\n",
    "\n",
    "### 2.3. Visualizing Layout Detection\n",
    "\n",
    "Visualize layout regions with color-coded boxes showing region ID, type, and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9168aa-7510-48bb-891e-a18bee934cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layout detection \n",
    "from matplotlib import colormaps\n",
    "\n",
    "def visualize_layout(image_path, layout_regions, min_confidence=0.5, \n",
    "                     title=\"Layout Detection\"):\n",
    "    \"\"\"\n",
    "    Visualize layout detection results using cv2 (same pattern as L2).\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_plot = img.copy()\n",
    "    \n",
    "    # Get unique labels and generate colors\n",
    "    labels = list(set(r.region_type for r in layout_regions))\n",
    "    cmap = colormaps.get_cmap('tab20')\n",
    "    color_map = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        rgba = cmap(i % 20)\n",
    "        color_map[label] = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))\n",
    "    \n",
    "    for region in layout_regions:\n",
    "        if region.confidence < min_confidence:\n",
    "            continue\n",
    "            \n",
    "        color = color_map[region.region_type]\n",
    "        x1, y1, x2, y2 = region.bbox\n",
    "        \n",
    "        # Draw rectangle\n",
    "        pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype=int)\n",
    "        cv2.polylines(img_plot, [pts], True, color, 2)\n",
    "        \n",
    "        # Add label\n",
    "        text = f\"{region.region_id}: {region.region_type} ({region.confidence:.2f})\"\n",
    "        cv2.putText(img_plot, text, (x1, y1-8), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.imshow(cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return img_plot\n",
    "\n",
    "visualize_layout(image_path, layout_regions, \n",
    "                 min_confidence=0.5, title=\"PaddleOCR Layout Detection\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-4",
   "metadata": {},
   "source": [
    "<a id=\"2-4\"></a>\n",
    "\n",
    "### 2.4. Cropping Regions for Agent Tools\n",
    "\n",
    "Prepare cropped regions for VLM analysis:\n",
    "- **Focused analysis** - VLM sees only relevant content\n",
    "- **Reduced noise** - No surrounding text interference\n",
    "- **Lower costs** - Smaller images reduce API costs\n",
    "\n",
    "Images are base64-encoded for vision API compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2a1a2-df03-4057-947f-f24dd7a3438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Crop and save layout regions for agent tools\n",
    "def crop_region(image, bbox, padding=10):\n",
    "    \"\"\"Crop a region from image with optional padding.\"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1 = max(0, x1 - padding)\n",
    "    y1 = max(0, y1 - padding)\n",
    "    x2 = min(image.width, x2 + padding)\n",
    "    y2 = min(image.height, y2 + padding)\n",
    "    return image.crop((x1, y1, x2, y2))\n",
    "\n",
    "def image_to_base64(img):\n",
    "    \"\"\"Convert PIL Image to base64 string.\"\"\"\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format='PNG')\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Load image for cropping\n",
    "pil_image = Image.open(image_path)\n",
    "\n",
    "# Store cropped regions in dictionary\n",
    "region_images = {}\n",
    "for region in layout_regions:\n",
    "    cropped = crop_region(pil_image, region.bbox)\n",
    "    region_images[region.region_id] = {\n",
    "        'image': cropped,\n",
    "        'base64': image_to_base64(cropped),\n",
    "        'type': region.region_type,\n",
    "        'bbox': region.bbox\n",
    "    }\n",
    "\n",
    "print(f\"Cropped {len(region_images)} regions\")\n",
    "\n",
    "# Also store full image\n",
    "full_image_base64 = image_to_base64(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viewing-crops-note",
   "metadata": {},
   "source": [
    "Display all cropped regions available to the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d14e8-91a6-498d-a0d8-7a6c51f96084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cropped regions\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (region_id, data) in enumerate(list(region_images.items())[:14]):\n",
    "    axes[i].imshow(data['image'])\n",
    "    axes[i].set_title(f\"Region {region_id}: {data['type']}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Agent Tools\n",
    "\n",
    "Create two specialized VLM tools with optimized prompts:\n",
    "- **AnalyzeChart**: Interpret charts and figures\n",
    "- **AnalyzeTable**: Extract structured table data\n",
    "\n",
    "Specialized tools enable content-specific prompts and structured JSON outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12949d42-64e4-4b20-b6d6-54e32edde66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize VLM for tools\n",
    "vlm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1",
   "metadata": {},
   "source": [
    "<a id=\"3-1\"></a>\n",
    "\n",
    "### 3.1. VLM Helper and Prompts\n",
    "\n",
    "Prompts define structured VLM output with three components:\n",
    "1. **Role definition** (e.g., \"Chart Analysis specialist\")\n",
    "2. **Extraction fields** (chart type, axes, data points)\n",
    "3. **JSON template** for consistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8a1b-c618-4135-aa86-fc0400b1de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool prompts\n",
    "CHART_ANALYSIS_PROMPT = \"\"\"You are a Chart Analysis specialist. \n",
    "Analyze this chart/figure image and extract:\n",
    "\n",
    "1. **Chart Type**: (line, bar, scatter, pie, etc.)\n",
    "2. **Title**: (if visible)\n",
    "3. **Axes**: X-axis label, Y-axis label, and tick values\n",
    "4. **Data Points**: Key values (peaks, troughs, endpoints)\n",
    "5. **Trends**: Overall pattern description\n",
    "6. **Legend**: (if present)\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "```json\n",
    "{{\n",
    "  \"chart_type\": \"...\",\n",
    "  \"title\": \"...\",\n",
    "  \"x_axis\": {{\"label\": \"...\", \"ticks\": [...]}},\n",
    "  \"y_axis\": {{\"label\": \"...\", \"ticks\": [...]}},\n",
    "  \"key_data_points\": [...],\n",
    "  \"trends\": \"...\",\n",
    "  \"legend\": [...]\n",
    "}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc481637-e450-4a78-ad12-20a9825a2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_ANALYSIS_PROMPT = \"\"\"You are a Table Extraction specialist. \n",
    "Extract structured data from this table image.\n",
    "\n",
    "1. **Identify Structure**: \n",
    "    - Column headers, row labels, data cells\n",
    "2. **Extract All Data**: \n",
    "    - Preserve exact values and alignment\n",
    "3. **Handle Special Cases**: \n",
    "    - Merged cells, empty cells (mark as null), multi-line headers\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "```json\n",
    "{{\n",
    "  \"table_title\": \"...\",\n",
    "  \"column_headers\": [\"header1\", \"header2\", ...],\n",
    "  \"rows\": [\n",
    "    {{\"row_label\": \"...\", \"values\": [val1, val2, ...]}},\n",
    "    ...\n",
    "  ],\n",
    "  \"notes\": \"any footnotes or source info\"\n",
    "}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlm-helper-note",
   "metadata": {},
   "source": [
    "**VLM Helper Function** \n",
    "\n",
    "Call VLM with multimodal messages containing prompt and base64-encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ce0db-39c5-45db-b9d5-085baecc3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_vlm_with_image(image_base64: str, prompt: str) -> str:\n",
    "    \"\"\"Call VLM with an image and prompt.\"\"\"\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    response = vlm.invoke([message])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "<a id=\"3-2\"></a>\n",
    "\n",
    "### 3.2. Creating the AnalyzeChart Tool\n",
    "\n",
    "The `@tool` decorator converts functions to agent-usable tools. This tool validates region existence, retrieves cropped images, and calls the VLM with the chart analysis prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0e556-2c36-4b0c-aba0-aebca04be3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def AnalyzeChart(region_id: int) -> str:\n",
    "    \"\"\"Analyze a chart or figure region using VLM. \n",
    "    Use this tool when you need to extract data from charts, graphs, or figures.\n",
    "    \n",
    "    Args:\n",
    "        region_id: The ID of the layout region to analyze (must be a chart/figure type)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with chart type, axes, data points, and trends\n",
    "    \"\"\"\n",
    "    if region_id not in region_images:\n",
    "        return f\"Error: Region {region_id} not found. Available regions: {list(region_images.keys())}\"\n",
    "    \n",
    "    region_data = region_images[region_id]\n",
    "    \n",
    "    if region_data['type'] not in ['chart', 'figure']:\n",
    "        return f\"Warning: Region {region_id} is type '{region_data['type']}', not a chart/figure. Proceeding anyway.\"\n",
    "    \n",
    "    result = call_vlm_with_image(region_data['base64'], CHART_ANALYSIS_PROMPT)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"AnalyzeChart tool defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-3",
   "metadata": {},
   "source": [
    "<a id=\"3-3\"></a>\n",
    "\n",
    "### 3.3. Creating the AnalyzeTable Tool\n",
    "\n",
    "Create the table extraction tool using the table-specific prompt for structured data with headers and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1c340-fe96-459f-bbb3-e128f492ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def AnalyzeTable(region_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract structured data from a table region using VLM.\n",
    "    Use this tool when you need to extract tabular data \n",
    "    with headers and rows.\n",
    "    \n",
    "    Args:\n",
    "        region_id: The ID of the layout region to analyze (must be a table type)\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with table headers, rows, and any notes\n",
    "    \"\"\"\n",
    "    if region_id not in region_images:\n",
    "        return f\"Error: Region {region_id} not found. Available regions: {list(region_images.keys())}\"\n",
    "    \n",
    "    region_data = region_images[region_id]\n",
    "    \n",
    "    if region_data['type'] != 'table':\n",
    "        return f\"Warning: Region {region_id} is type '{region_data['type']}', not a table. Proceeding anyway.\"\n",
    "    \n",
    "    result = call_vlm_with_image(region_data['base64'], TABLE_ANALYSIS_PROMPT)\n",
    "    return result\n",
    "\n",
    "print(\"AnalyzeTable tool defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-4",
   "metadata": {},
   "source": [
    "<a id=\"3-4\"></a>\n",
    "\n",
    "### 3.4. Testing the Tools\n",
    "\n",
    "Test tools individually to verify VLM connection and prompt effectiveness.\n",
    "\n",
    "Analyze the first chart region. Data points are approximateâ€”VLMs have limitations in precise visual localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f9fd8-6b3f-4a70-9c4c-11c12653a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tools\n",
    "print(\"Testing AnalyzeChart...\")\n",
    "chart_regions = [r for r in layout_regions if r.region_type in ['chart', 'figure']]\n",
    "if chart_regions:\n",
    "    test_result = AnalyzeChart.invoke({\"region_id\": chart_regions[0].region_id})\n",
    "    print(f\"Chart analysis result:\\n{test_result[:500]}...\")\n",
    "else:\n",
    "    print(\"No chart regions found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-table-note",
   "metadata": {},
   "source": [
    "Test the table tool. Works well for simple tables but struggles with complex layouts and may hallucinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3f8b9-e6ed-4020-9b1d-0e84944f4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test table tool\n",
    "print(\"Testing AnalyzeTable...\")\n",
    "table_regions = [r for r in layout_regions if r.region_type == 'table']\n",
    "if table_regions:\n",
    "    test_result = AnalyzeTable.invoke({\"region_id\": table_regions[0].region_id})\n",
    "    print(f\"Table analysis result:\\n{test_result[:500]}...\")\n",
    "else:\n",
    "    print(\"No table regions found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. LangChain Agent\n",
    "\n",
    "Build the agent to orchestrate all components:\n",
    "1. Receive question about document\n",
    "2. Read system prompt with OCR text and layout info\n",
    "3. Decide whether to answer from text or use tools\n",
    "4. Call appropriate tools for visual content\n",
    "5. Combine information into coherent response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-structures-note",
   "metadata": {},
   "source": [
    "Verify data structures: ordered text (OCR + LayoutLM) and layout regions (layout detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30405140-78dc-44b3-a7fe-b964ea71714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515e41d-5b95-4f61-b01b-2637de4d88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_regions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-1",
   "metadata": {},
   "source": [
    "<a id=\"4-1\"></a>\n",
    "\n",
    "### 4.1. Formatting Context for the Agent\n",
    "\n",
    "Convert data structures to readable text for the system promptâ€”the agent's \"memory\" of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec4fef-991a-4596-af9f-9f6afdd7dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare context for the agent\n",
    "def format_ordered_text(ordered_text, max_items=50):\n",
    "    \"\"\"Format ordered text for the system prompt.\"\"\"\n",
    "    lines = []\n",
    "    for item in ordered_text[:max_items]:\n",
    "        lines.append(f\"[{item['position']}] {item['text']}\")\n",
    "    \n",
    "    if len(ordered_text) > max_items:\n",
    "        lines.append(f\"... and {len(ordered_text) - max_items} more text regions\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def format_layout_regions(layout_regions):\n",
    "    \"\"\"Format layout regions for the system prompt.\"\"\"\n",
    "    lines = []\n",
    "    for region in layout_regions:\n",
    "        lines.append(f\"  - Region {region.region_id}: {region.region_type} (confidence: {region.confidence:.3f})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Create the formatted strings\n",
    "ordered_text_str = format_ordered_text(ordered_text)\n",
    "layout_regions_str = format_layout_regions(layout_regions)\n",
    "\n",
    "print(\"Formatted context for agent:\")\n",
    "print(f\"- Ordered text: {len(ordered_text_str)} chars\")\n",
    "print(f\"- Layout regions: {len(layout_regions_str)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-2",
   "metadata": {},
   "source": [
    "<a id=\"4-2\"></a>\n",
    "\n",
    "### 4.2. Creating the System Prompt\n",
    "\n",
    "Construct the system prompt with:\n",
    "- **Role definition**: Document Intelligence Agent\n",
    "- **Document context**: OCR text in reading order\n",
    "- **Layout information**: Region types and IDs\n",
    "- **Tool descriptions**: When to use each tool\n",
    "- **Instructions**: How to handle different content types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17678b-7803-447e-a944-348e861d6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the agent\n",
    "SYSTEM_PROMPT = f\"\"\"You are a Document Intelligence Agent. \n",
    "You analyze documents by combining OCR text with visual analysis tools.\n",
    "\n",
    "## Document Text (in reading order)\n",
    "The following text was extracted using OCR and ordered using LayoutLM.\n",
    "\n",
    "{ordered_text_str}\n",
    "\n",
    "## Document Layout Regions\n",
    "The following regions were detected in the document:\n",
    "\n",
    "{layout_regions_str}\n",
    "\n",
    "## Your Tools\n",
    "- **AnalyzeChart(region_id)**: \n",
    "    - Use for chart/figure regions to extract data points, axes, and trends\n",
    "- **AnalyzeTable(region_id)**: \n",
    "    - Use for table regions to extract structured tabular data\n",
    "\n",
    "## Instructions\n",
    "1. For TEXT regions: \n",
    "    - Use the OCR text provided above (it's already extracted)\n",
    "2. For TABLE regions: \n",
    "    - Use the AnalyzeTable tool to get structured data\n",
    "3. For CHART/FIGURE regions: \n",
    "    - Use the AnalyzeChart tool to extract visual data\n",
    "\n",
    "When answering questions about the document, \n",
    "use the appropriate tools to get accurate information.\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt created\")\n",
    "print(f\"Total length: {len(SYSTEM_PROMPT)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-3",
   "metadata": {},
   "source": [
    "<a id=\"4-3\"></a>\n",
    "\n",
    "### 4.3. Assembling the Agent\n",
    "\n",
    "Assemble the agent using `create_tool_calling_agent`:\n",
    "- **Tools**: AnalyzeChart and AnalyzeTable\n",
    "- **LLM**: GPT-4o-mini for cost efficiency\n",
    "- **Prompt**: System context + user input + agent scratchpad\n",
    "- **Verbose mode**: Shows agent reasoning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bf401-462f-4ffa-9d20-abc2f2cb4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent (using LangChain 0.1.x API)\n",
    "tools = [AnalyzeChart, AnalyzeTable]\n",
    "\n",
    "# LLM for the agent \n",
    "agent_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",SYSTEM_PROMPT),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Create the tool-calling agent\n",
    "agent = create_tool_calling_agent(agent_llm, tools, prompt)\n",
    "\n",
    "# 5. Set up the AgentExecutor to run the tool-enabled loop\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-4",
   "metadata": {},
   "source": [
    "<a id=\"4-4\"></a>\n",
    "\n",
    "### 4.4. Testing the Agent\n",
    "\n",
    "Test the agent with three question types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diff-results-note",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their non-deterministic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test1-note",
   "metadata": {},
   "source": [
    "**Test 1: Document Overview**\n",
    "\n",
    "Ask a general question answerable from OCR text alone (no tool calls needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c68ff2-8ff7-40d2-8f07-7d1532efc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple question\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"What types of content are in this document?\"\n",
    "              \"List the main sections.\",\n",
    "})\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Agent Response:\")\n",
    "print(\"=\"*60)\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test2-note",
   "metadata": {},
   "source": [
    "**Test 2: Table Data Extraction**\n",
    "\n",
    "Extract table data by calling the `AnalyzeTable` tool. Verbose output shows reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144488a-6986-49cd-b211-df6ca0d5a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with table extraction\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"Extract the data from the table in this document.\" \n",
    "             \"Return it in a structured format.\"})\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Agent Response:\")\n",
    "print(\"=\"*60)\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test3-note",
   "metadata": {},
   "source": [
    "**Test 3: Chart Analysis**\n",
    "\n",
    "Analyze the chart using the `AnalyzeChart` tool to extract visual information unavailable from OCR text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37b96a-8690-4ba1-8686-f2c97ff32b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with chart analysis\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": \"Analyze the chart/figure in this document.\" \n",
    "    \"What trends does it show?\"})\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Agent Response:\")\n",
    "print(\"=\"*60)\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Our hybrid approach breaks documents into separate regions of text, charts or tables. The LangChain agent uses different tools for each region. \n",
    "\n",
    "| Component | Purpose | Output |\n",
    "|-----------|---------|--------|\n",
    "| **PaddleOCR** | Text Parsing | Text + bounding boxes|\n",
    "| **LayoutReader** | Reading order prediction | Sorted sequence of regions |\n",
    "| **PaddleOCR** | Layout Detection | Region types (table, chart, text) |\n",
    "| **VLM** | Analysis of charts/tables | JSON (title, legend,... / headers, rows,...) |\n",
    "\n",
    "In the next lesson, you will study the **Agentic Document Extraction (ADE)** framework from LandingAI. It will handle text parsing, layout detection, reading order, multimodal reasoning, and schema-based extraction in unified API's. This will address several limitations of PaddleOCR on real-world documents. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
